{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 神经网络的学习\n",
    "常规流程:\n",
    "1. 先进行学习\n",
    "    寻找最优参数\n",
    "2. 再进行推理\n",
    "    利用学习好的参数对分类等问题给出答案\n",
    "\n",
    "## 损失函数\n",
    "\n",
    "**损失(loss)**：评价神经网络学习中某阶段某时间点的一个性能指标。基于 监督数据 和 预测结果。\n",
    "\n",
    "**损失函数(loss function)**：计算神经网络损失的函数。\n",
    "\n",
    "\n",
    "使用了损失函数的神经网络的层结构\n",
    "\n",
    "**softmax函数**\n",
    "\n",
    "**one-hot向量**\n",
    "\n",
    "**交叉熵误差**\n",
    "\n",
    "**mini-batch交叉熵误差**\n",
    "\n",
    "## 导数和梯度\n",
    "\n",
    "**导数**\n",
    "\n",
    "**偏导数**\n",
    "\n",
    "**梯度** 适用向量、矩阵\n",
    "- 数学领域\n",
    "- 深度学习领域\n",
    "\n",
    "## 链式法则\n",
    "\n",
    "**复合函数**\n",
    "\n",
    "**链式法则**\n",
    "\n",
    "## 计算图\n",
    "\n",
    "**误差反向传播法**\n",
    "\n",
    "### 加法节点\n",
    "### 乘法节点\n",
    "### 分支节点（复制节点）\n",
    "### Repeat节点\n",
    "### Sum节点\n",
    "通用的解法节点， 和Repeat节点为逆向关系。\n",
    "### MatMul节点\n",
    "矩阵乘积（Matrix Multiply）\n",
    "\n",
    "## 梯度的推导、反向传播的实现\n",
    "\n",
    "### Sigmoid层\n",
    "### Affine层\n",
    "### Softmax with Loss层\n",
    "\n",
    "## 权重的更新\n",
    "神经网络的学习步骤：\n",
    "1. mini-batch\n",
    "2. 计算梯度\n",
    "    基于误差反向传播法，计算函数损失关于各个权重参数的梯度。\n",
    "3. 更新参数\n",
    "    使用梯度更新权重参数。\n",
    "4. 重复\n",
    "\n",
    "**梯度下降法（gradient descent）**: 将参数向梯度反方向更新，降低损失。\n",
    "\n",
    "**随机梯度下降法（Stochastic Gradient Descent, SGD）** “随机”指随机选择 mini-batch 数据\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ba04d15bdb7c097"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
